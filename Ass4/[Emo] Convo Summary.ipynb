{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convo Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we loook at different types of LLMS and how they summarize audio transcripts. Specifically we are interested in:\n",
    "- How concise each model summarizes the transcripts to\n",
    "- How understandable the summaries are- measured in terms of how quickly a receiver may view some measurement of \"urgency\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import os\n",
    "import openai\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We store a list of prompts to reuse for every model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concise_prompt = \"Summarize the following audio transcript in 3–4 sentences. Focus on the main issue discussed, the speaker’s emotional state, and any signs of urgency or distress\"\n",
    "analytical_prompt = \"Read this conversation transcript and provide a concise summary that identifies the key concern, emotional tone, and any notable escalation or risk indicators. Use professional and neutral language.\"\n",
    "structured_prompt = \"Summarize the transcript with the following format:\\n- Main issue: \\n- Emotional state: \\n- Urgency level (low/medium/high):\"\n",
    "\n",
    "list_of_prompts = [concise_prompt, analytical_prompt, structured_prompt]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ChatGPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by looking into summaries generated by ChatGPT 4o."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the API Key, stored in seperate environment file\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to prompt ChatGPT\n",
    "def get_chatgpt_response(prompt):\n",
    "    completion = openai.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "    return completion.choices[0].message.content\n",
    "\n",
    "\n",
    "def store_chatgpt_responses(list_of_prompts):\n",
    "    # Loop through each prompt\n",
    "    for prompt_num, prompt in enumerate(list_of_prompts, start=1):\n",
    "        # Define output path for this prompt's results\n",
    "        output_filename = f\"Summaries/gpt-4/prompt{prompt_num}_summary.txt\"\n",
    "        \n",
    "        # Create/open the file for writing all call summaries under this prompt\n",
    "        with open(output_filename, \"w\", encoding=\"utf-8\") as out_file:\n",
    "            # Loop through all transcripts (call01–call05)\n",
    "            for i in range(1, 6):\n",
    "                filename = f\"call{i:02d}.txt\"\n",
    "                filepath = f\"Transcripts/{filename}\"  # based on your folder structure\n",
    "\n",
    "                with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "                    transcript_text = f.read()\n",
    "\n",
    "                # Combine prompt with transcript\n",
    "                full_prompt = f\"{prompt}\\n\\nTranscript:\\n{transcript_text}\"\n",
    "                response = get_chatgpt_response(full_prompt)\n",
    "\n",
    "                # Write formatted summary to the output file\n",
    "                out_file.write(f\"=== Summary for {filename} ===\\n\")\n",
    "                out_file.write(response.strip() + \"\\n\\n\")  # add space between summaries\n",
    "\n",
    "            print(f\"Saved all summaries for prompt {prompt_num} to {output_filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved all summaries for prompt 1 to Summaries/gpt-4/prompt1_summary.txt\n",
      "Saved all summaries for prompt 2 to Summaries/gpt-4/prompt2_summary.txt\n",
      "Saved all summaries for prompt 3 to Summaries/gpt-4/prompt3_summary.txt\n"
     ]
    }
   ],
   "source": [
    "store_chatgpt_responses(list_of_prompts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In each txt file, we are able to see how each prompt summarizes the five different audio transcripts. We observe that the **structured_prompt** does the best in summarizing in **the most organized/easily understandable manner**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "At least one of TensorFlow 2.0 or PyTorch should be installed. To install TensorFlow 2.0, read the instructions at https://www.tensorflow.org/install/ To install PyTorch, read the instructions at https://pytorch.org/.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[51], line 19\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Loop over each model\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model_name, model_id \u001b[38;5;129;01min\u001b[39;00m HUGGINGFACE_MODELS\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m---> 19\u001b[0m     summarizer \u001b[38;5;241m=\u001b[39m pipeline(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msummarization\u001b[39m\u001b[38;5;124m\"\u001b[39m, model\u001b[38;5;241m=\u001b[39mmodel_id)\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m prompt_num, prompt \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(list_of_prompts, start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m     22\u001b[0m         output_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSummaries/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/prompt\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprompt_num\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_summary.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\emily\\anaconda3\\Lib\\site-packages\\transformers\\pipelines\\__init__.py:942\u001b[0m, in \u001b[0;36mpipeline\u001b[1;34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[0;32m    940\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m framework \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    941\u001b[0m     model_classes \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m]}\n\u001b[1;32m--> 942\u001b[0m     framework, model \u001b[38;5;241m=\u001b[39m infer_framework_load_model(\n\u001b[0;32m    943\u001b[0m         adapter_path \u001b[38;5;28;01mif\u001b[39;00m adapter_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m model,\n\u001b[0;32m    944\u001b[0m         model_classes\u001b[38;5;241m=\u001b[39mmodel_classes,\n\u001b[0;32m    945\u001b[0m         config\u001b[38;5;241m=\u001b[39mconfig,\n\u001b[0;32m    946\u001b[0m         framework\u001b[38;5;241m=\u001b[39mframework,\n\u001b[0;32m    947\u001b[0m         task\u001b[38;5;241m=\u001b[39mtask,\n\u001b[0;32m    948\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs,\n\u001b[0;32m    949\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m    950\u001b[0m     )\n\u001b[0;32m    952\u001b[0m model_config \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\n\u001b[0;32m    953\u001b[0m hub_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_commit_hash\n",
      "File \u001b[1;32mc:\\Users\\emily\\anaconda3\\Lib\\site-packages\\transformers\\pipelines\\base.py:242\u001b[0m, in \u001b[0;36minfer_framework_load_model\u001b[1;34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[0m\n\u001b[0;32m    216\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;124;03mSelect framework (TensorFlow or PyTorch) to use from the `model` passed. Returns a tuple (framework, model).\u001b[39;00m\n\u001b[0;32m    218\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    239\u001b[0m \u001b[38;5;124;03m    `Tuple`: A tuple framework, model.\u001b[39;00m\n\u001b[0;32m    240\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    241\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tf_available() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_available():\n\u001b[1;32m--> 242\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    243\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAt least one of TensorFlow 2.0 or PyTorch should be installed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    244\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo install TensorFlow 2.0, read the instructions at https://www.tensorflow.org/install/ \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    245\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo install PyTorch, read the instructions at https://pytorch.org/.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    246\u001b[0m     )\n\u001b[0;32m    247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    248\u001b[0m     model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_from_pipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m task\n",
      "\u001b[1;31mRuntimeError\u001b[0m: At least one of TensorFlow 2.0 or PyTorch should be installed. To install TensorFlow 2.0, read the instructions at https://www.tensorflow.org/install/ To install PyTorch, read the instructions at https://pytorch.org/."
     ]
    }
   ],
   "source": [
    "# Define the models you want to use\n",
    "HUGGINGFACE_MODELS = {\n",
    "    \"bart\": \"facebook/bart-large-cnn\",\n",
    "    \"t5\": \"t5-base\"\n",
    "}\n",
    "# Loop over each model\n",
    "for model_name, model_id in HUGGINGFACE_MODELS.items():\n",
    "    summarizer = pipeline(\"summarization\", model=model_id)\n",
    "\n",
    "    for prompt_num, prompt in enumerate(list_of_prompts, start=1):\n",
    "        output_path = f\"Summaries/{model_name}/prompt{prompt_num}_summary.txt\"\n",
    "        os.makedirs(f\"Summaries/{model_name}\", exist_ok=True)\n",
    "\n",
    "        with open(output_path, \"w\", encoding=\"utf-8\") as out_file:\n",
    "            for i in range(1, 6):  # call01 to call05\n",
    "                filename = f\"call{i:02d}.txt\"\n",
    "                filepath = f\"Transcripts/{filename}\"\n",
    "\n",
    "                with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "                    transcript_text = f.read()\n",
    "\n",
    "                # Create the input to the summarization model\n",
    "                input_text = f\"{prompt}\\n\\nTranscript:\\n{transcript_text}\"\n",
    "\n",
    "                # Hugging Face models usually limit input to ~1024–2048 tokens\n",
    "                input_trimmed = input_text[:1024]  # Trim to avoid token cutoff\n",
    "                summary = summarizer(input_trimmed, max_length=150, min_length=30, do_sample=False)[0][\"summary_text\"]\n",
    "\n",
    "                out_file.write(f\"=== Summary for {filename} ===\\n\")\n",
    "                out_file.write(summary.strip() + \"\\n\\n\")\n",
    "\n",
    "            print(f\"[{model_name.upper()}] Saved summaries for prompt {prompt_num} to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
